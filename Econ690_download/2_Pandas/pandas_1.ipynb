{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas and packages\n",
    "\n",
    "files needed = none\n",
    "\n",
    "We have a handle on python now: We understand the data structures and enough about working with them to move on to stuff more directly relevant to data analysis. That means digging into **pandas** the package that provides the core data structures and functions for data work. \n",
    "\n",
    "Pandas is a package. We have seen the numpy and scipy packages before, but we haven't discussed packages in any detail yet. Let's brush up on packages before we jump into pandas.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "\n",
    "We can think of 'python' as a core set of libraries (the python standard libraries) that contain the basic python data structures (`int`, `str`, `list`, `dict`, etc.) and functions (`print()`, `len()`, `sum()`, etc). We can add additional data structures and functions to core python. Developers group related functions and data structures into *packages* which we add to core python to increase its functionality. \n",
    "\n",
    "On winstat, we are working with the [Anaconda](https://www.anaconda.com/download) *distribution* of python. The Anaconda distribution bundles together basic python with common and useful packages such as\n",
    "* numpy: functions for numerical computing (ln(), exp(), sqrt(),...) \n",
    "* scipy: functions for math and engineering (statistical functions, signal processing, linear algebra...)\n",
    "* matplotlib: a plotting interface (bar, scatter, time series plots,...)\n",
    "* pandas: functions and data structures for data analysis (data management, merging, grouping...)\n",
    "\n",
    "At the end of the semester, when you move away from winstat, you can install Anaconda (it's free) on your own computer and continue your python adventures. \n",
    "\n",
    "### Installing packages vs. importing packages\n",
    "Anaconda already *installed* the packages for us --- that is, downloaded and installed the packages onto our computers.  Once the package is installed, we still need to tell python that we want to use the packages in our program. This way, we only load into memory the packages we need. \n",
    "\n",
    "To add a package we use the `import` statement. For example, to load the numpy package, we use:\n",
    "```python\n",
    "import numpy as np\n",
    "```\n",
    "The statement says to add the numpy package (import it) and to give it a shortened name of 'np'. We don't have to use the name np --- we could name it George --- but np is short and informative. \n",
    "\n",
    "Why do we want a shorter name than numpy? When we use a function from the numpy package, we use the dot notation. Here, we use the [log function](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html):\n",
    "```python\n",
    "y = 10\n",
    "x = np.log(y)\n",
    "```\n",
    "The `np.` tells python to look for the `log()` function in the package called np. Python knows that np means numpy, and everything works. \\[See why calling it George would have looked strange?\\]\n",
    "\n",
    "We only need to import a package once per notebook. We typically group all of our imports at the beginning of the code. Here we go:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd     #load the pandas package and call it pd\n",
    "import numpy as np      #load the pandas package and call it np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the cell, you might notice that the text to the left of the cell looked like `In[*]` for a while. That * means that python was working. It takes a few seconds to load all those functions. Now run `whos` to see what's in the namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see two `module` variables ready for action. As usual, we can use `?` to check them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`random` seems useful. Let's see what's in there..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some random numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.random_sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full list of the packages included in the Anaconda distribution is [here](https://docs.anaconda.com/anaconda/packages/py3.6_win-64/). If we need a package that is not already a part of Anaconda, we will need to install it. More on that later, but the **big idea** is that python can be extended by installing and adding packages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "Pandas (*pan*el *da*ta) is our workhorse package for data anlaysis. In addition to all the things we can do with core python, pandas gives us many  powerful functions and data types that we can use for working with data. If you are familiar with STATA, you will find many parallels between python and STATA. This makes sense: both are meant to handle panel data. Pandas was invented by Wes McKinney (sound familiar?) at the quantitative hedge fund AQR. One of the reasons we have chosen python for this course is its prevalence in the financial sector. \n",
    "\n",
    "To make the pandas package available, we `import` it into our code. We have already imported it above, but we can do it again. It won't hurt.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # The most common short name for pandas is pd. We will go with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas data structures: DataFrames\n",
    "Pandas is built around two main data structures: the *series* and the *dataframe*. A dataframe is a 'rectangular' data structure, like a Microsoft Excel spreadsheet. It is made up of rows and columns. \\[But it is much, much, more powerful than a spreadsheet.\\] \n",
    "\n",
    "Let's create a dataframe. To keep things simple (they will get complicated later!) we will create a dataframe from a dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {'year': [1990, 1995, 2000, 2005 ], 'gdp':[5.9, 7.6, 10.2, 13.0], 'cons':[3.8, 4.9, 6.8, 8.7]}\n",
    "print('First, print the dict:')\n",
    "print(data_dict)             # print the dict\n",
    "\n",
    "df = pd.DataFrame(data_dict)  # create a data frame called df from the dict. \n",
    "                              # use the 'pd.' syntax to call the DataFrame constructor \n",
    "\n",
    "print('\\nNext, print the DataFrame and its type\\n')\n",
    "print(df)                     # Print the dataframe. Again, print() just knows what to do!\n",
    "print('\\n', type(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. Compared to the dict, the dataframe is much easier to read when printed. Let's break it down: \n",
    "```python\n",
    "df = pd.DataFrame(data_dict)\n",
    "```\n",
    "We are creating a DataFrame object. We can see that when we print out df's type. The stuff to the left of DataFrame in the print out is the hierarchy: `DataFrame` is part of the `frame` group which is part of the `core` group of the package `pandas`. The hierarchy is not important for our work.\n",
    "\n",
    "We call the `DataFrame()` (note the caps) function from the pandas (`pd.`) package. The `DataFrame()` function creates dataframes from other objects. If we don't pass an argument, it creates an empty DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame()\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this function sometimes, but, more often, we will be building our DataFrames from spreadsheets, csv files, or api (application programming interface) calls directly to servers to retrive data. Very powerful stuff.  \n",
    "\n",
    "### Columns and rows in a DataFrame\n",
    "A DataFrame is made up of columns and rows. How big is our DataFrame? We use the `shape` attribute of a DataFrame to see this. \\[Reminder: *attributes* and *methods* are associated with objects. Attributes return features of the object and methods are like functions that act on the object.\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The shape is:', df.shape)   # shape returns the (rows, columns) of the DataFrame\n",
    "\n",
    "print('The size is:', df.size)    # I often mess up and try this when I want the 'size' of a DataFrame. What is it telling us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What type does `df.shape` return? \n",
    "\n",
    "Back to our DataFrame. Let' print if out again, but this time, without the print function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The jupyter notebook adds some shading and formatting, making it even easier to read. I still often use the `print()` function. Old habits die hard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DataFrame Index\n",
    "The left-most 'column' of numbers is not a column. It is the **index** that pandas assigned to our DataFrame: similar to the row 'numbers' of a spreadsheet. I put numbers in quotes, because an index can be more than just a sequence of integers. In this DataFrame, it makes sense for our index to be the year variable. A little later, we will discuss changing the index. For now, we will go with what pandas gave us.  \n",
    "\n",
    "The index is important; we will work on in more in a bit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas data structures: Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reference a column in the DataFrame by its name. Notice the similarity to the syntax for referencing a dict key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['gdp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we print the column, we get the index, the column, and the type of data contained in the column. 'gdp' are floats. Suppose we instead ask for the type of the column itself --- not what's in the column. This reveals the second major data structure in  pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp = df['gdp']\n",
    "print(gdp)\n",
    "print(type(gdp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we extract a single column from a DataFrame, we are given a **Series**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df['gdp']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of a Series as a DataFrame with only one variable: It is one column and an index. In the same way, we can think of a DataFrame as a collection of Series that all have the same index. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning more about a DataFrame\n",
    "We have seen the `size` and `shape` methods of DataFrame. Let's look at a few more. If we want to see what variables we have in the DataFrame (imagine you are working with a dataset containing 1,000 variables...) we use the `columns` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print(type(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the column attribute returns the list of columns in a somewhat complicated way. (It is returning an Index object that is useful in some situations.)  We can use the `tolist()` method to create a list object. We can do something similar to the index as well. The `axes` attribute tells us about both at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist())\n",
    "\n",
    "print(df.index.tolist())\n",
    "\n",
    "print(df.axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can learn about the data types in DataFrame with `dtypes` \\[note the 's'\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice: DataFrames\n",
    "Take a few minutes and try the following. Feel free to chat with those around you if you get stuck. The TA and I are here, too. \n",
    "\n",
    "Below is a dict with data about U.S. states. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_data = {'state':['CA','MI','WI','MN'], 'pop':[37,9.8,5.7, '5.3'], 'size':[163.7, 96.7, 65.5, 86.9], 'bird':['Quail', 'Redbreast Robin', 'American Robin', 'Loon']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convert the dict to a DataFrame named `states`\n",
    "2. What are the dtypes of the variables? Does anything look funny?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How many rows and columns are in this DataFrame? \n",
    "4. Print out the index as a list. Is there a variable that might make a more sensible index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on dataframe columns\n",
    "To extract a column from a DataFrame, we have been using `states['bird']`. This is the preferred way to do so, in that it always works. You will come accros two other ways to extract a colulmn. \n",
    "1. The first uses a `.` like: `states.bird`. \n",
    "2. The second uses the `iloc` method `states.iloc[:,3]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states['bird'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states.bird)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states.iloc[:,3])   #iloc works like a 2-D slice with iloc[rows, cols]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three returned the same column. Why do I not recommend the second two approaches?  \n",
    "\n",
    "Let's try to grab the size column using the second method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states.size)   # Ask for the column 'size'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's not what I expected. On further review, however, it is exactly what I should have expected. `size` is an attribute of DataFrame. We saw this earlier: it returns the number of elements in the DataFrame. Any column name that conflicts with a DataFrame method or attribute is going to cause problems. \n",
    "\n",
    "What about `iloc`? Suppose I decide to rearrange the order of the columns. (We will see why the following code works shortly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = states[['state', 'size', 'bird', 'pop']]\n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this statement return?\n",
    "print(states.iloc[:,3])   #iloc works like a 2-D slice with iloc[rows, cols]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, this way of referencing a column is not robust to reordering the columns. We might still use `iloc` here and there, but we do so at our own peril. \n",
    "\n",
    "### Subsets of columns\n",
    "\n",
    "Okay, back to referencing columns. We can take several columns by passing a list of the column names.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_get = ['state', 'bird', 'pop']\n",
    "got_cols = states[cols_to_get]\n",
    "got_cols\n",
    "type(got_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we take more than one column, we are creating a DataFrame object. A more compact notation creates the list in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "got_cols_2 = states[['state', 'bird', 'pop']]\n",
    "got_cols_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming columns\n",
    "Often data sets come with poor variable names. How do we rename a column?\n",
    "If we are only changing a few variables, the dictionary approach works well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is it population or state soda? Let's get a better name on that variable. \n",
    "states = states.rename(columns={'pop':'population'})\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only column that was changed was the `pop` column. Let's take a closer look at the syntax. \n",
    "```python\n",
    "states = states.rename(columns={'pop':'population'})\n",
    "```\n",
    "1. We are calling the `rename())` method from the DataFrame object \n",
    "2. We pass `rename()`  the argument `columns` (passing `index` would rename the index)\n",
    "3. We use a dict to give the `{old name : new name}` key-value pairs\n",
    "\n",
    "Notice that we had to assign the result of `state.rename()` back to states. The `rename()` method does not act on the original data, but creates a copy, which we assign back to the `states` variable. \n",
    "\n",
    "We can ask `rename()` to perform the action on the original data with `inplace` argument. \\[You can see if a method supports in place operations by [checking the documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html), or using ? in jupyter notebook.\\] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's rename population again...\n",
    "states.rename(columns={'population':'people'}, inplace=True)\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need to apply a method or function to all the column names, we can operate directly on `df.columns`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder, what does df.columns return?\n",
    "print(states.columns)\n",
    "\n",
    "# All caps\n",
    "states.columns = [col_name.upper() for col_name in states.columns]\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that the `columns` attribute returns an iterable object. Like a range or a list, `states.columns` knows to dole out the column names when queired by a for loop. Nice.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on DataFrame Rows\n",
    "How do we take a row, or a subset of rows? We can resort to `iloc` again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states.iloc[2,:])      #Take the third row, all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...but this is still subject to problems with reordering rows. It is also less likely that we want to take a row in a specific position. It is more likely we want a row corresponding to a conditional. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states[states['STATE']=='WI']   # take the row corresponding to Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break that down. First, we ask which rows have STATE equal to WI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( states['STATE']=='WI' )\n",
    "print(type(states['STATE']=='WI'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is returning a Series of bools. When we give the Series to the DataFrame, it only grabs the rows that are true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the row index\n",
    "We can simplify our lives and get rid of an index that doesn't tell us anything useful about the data by changing the index.  The unit of observation in `states` is a state. That seems like a good index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_new_index = states.set_index('STATE')   # make the column 'STATE' the new index\n",
    "                                                 # assign the newly indexed DF to a new variable\n",
    "print(states_new_index)\n",
    "print('\\n')\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index is now the state abreviations and not the squence of integers. What does this buy us? A simpler way to reference a column using the `loc` method. This is **loc**, not iloc!\n",
    "\n",
    "\\[Again, notice that `set_index` did not operate in place. It created a copy that I assigned to the new variable. `set_index` also takes the `inplace` argument.\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states_new_index.loc['WI'])   # ask for the row corresponding to WI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very nice. Suppose you regret your choice of index. Are you ruined? Nope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_undo = states_new_index.reset_index()    # reset the index to integers and return state to a column\n",
    "states_undo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting rows and columns\n",
    "To remove a row or column, we use the `drop()` method. The drop method is our first introdution to the **axis** argument. DataFrames have two axes. We have been calling them rows and columns, but pandas thinks of them as rows = 0 and columns = 1. \n",
    "\n",
    "In the following we want to remove the column 'SIZE', but there could be a row index 'SIZE', too, so we need to tell drop where to look for 'SIZE'. Again, we use inplace if we don't want to create a copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_subset = states.drop('SIZE', axis = 1)\n",
    "states_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting a row works just as you would expect..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_no_mn = states.drop(3, axis = 0)   # Why is this code not very robust?\n",
    "states_no_mn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice: DataFrames\n",
    "Take a few minutes and try the following. Feel free to chat with those around you if you get stuck. The TA and I are here, too. \n",
    "\n",
    "1. Explain why `states = states[['state', 'size', 'bird', 'pop']]` reordered the DataFrame `states`.\n",
    "2. Create a new DataFrame from the dict `data_dict` we used earlier. Name it data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Change the index to be the year variable and print the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Change the remaining column names to 'consumption' and 'gross domestic product'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Change column names to title case. Try the string method `title()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Extract the rows corresponding to 1990 and 2000 and assign them to a DataFrame named `data_dec`. Hint: When we wanted to take to columns, we passed a list of the column names..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Create `data_dec_2` from data by deleting the unneeded rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculations on DataFrames\n",
    "\n",
    "A DataFrame naturally understand how to perform operatons element-wise. For example, let's compute the share of consumption in GDP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data_dict)    # reset data\n",
    "\n",
    "data['cons_share'] = data['cons'] / data['gdp'] # this will divide cons by gdp for each row\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the left-hand side of the assignment. I am creating a new variable (column) in the DataFrame and assigning it the consumption share. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oops, I wanted it in percentage\n",
    "\n",
    "data['cons_share'] = data['cons_share']*100\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The +,-,/,* operators all work element wise. As we have seen, multiplying and dividing by scaler works fine, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame methods for simple operations\n",
    "DataFrame has many methods for computing various statistics on the data. Note that some of them take an axis argument: you could compute `sum()` across a row or a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sums \n",
    "print('Sum across columns')\n",
    "print(data.sum(axis=1)) # summing across columns. Not terribly useful here.\n",
    "\n",
    "print('\\nSum across rows')\n",
    "print(data.sum(axis=0)) # summing across columns. Cumulative GDP, consumption\n",
    "\n",
    "print('\\nSum up gdp')\n",
    "print(data['gdp'].sum(axis=0)) # Sum a single column.\n",
    "\n",
    "# Means\n",
    "print('\\nMean of each column')\n",
    "print(data.mean(axis=0)) \n",
    "\n",
    "print('\\nMean gdp and cons')\n",
    "print(data[['gdp', 'cons']].mean(axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try TAB completion to see the mthods available or the [documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html).\n",
    "\n",
    "Here are a few: sum, mean, var, std, skew, rank, quantile, mode, min, max, kurtosis, cumsum, cumprod...\n",
    "\n",
    "These will be even more powerful once we learn how to group data within a DataFrame and compute statistics by group.\n",
    "\n",
    "One very useful one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.describe())   # a good place to start with a new data set\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice: DataFrames\n",
    "Take a few minutes and try the following. Feel free to chat with those around you if you get stuck. The TA and I are here, too. \n",
    "\n",
    "1. Compute the mean of the consumption share for 1990 and 1995. You might try using `loc[]` with two arguments `loc[rows, cols]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Try `desc = data.describe()` What is the return type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Looking ahead, try out the following code. What does it do? Can you find the file? What is inside of it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc.to_csv('desc.csv')\n",
    "desc.to_excel('desc.xlsx')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
